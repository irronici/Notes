# 无损压缩算法

![image-20200427093117062](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200427093117062.png)

[toc]

### 基本概念

+ 信息熵：设一个信号源$S＝｛s_1, s_2, … , s_n｝$，第*i*个符号的出现概率为$p_i$，信号源的熵为：
  $$
  \eta=H(S)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}
  $$

+ 平均码长：编码方法。设第i个符号的码字长度为$L_i$，则该信号源的平均码字长度$L$为
  $$
  L=\sum_{i=1}^{n} L_{i} * p_{i}
  $$

  + 平均码长不小于信息熵，平均码长最好接近信息熵，越接近编码效率越高

+ 数据压缩技术的性能指标
  + 压缩比 
  + 算法复杂度 

+ 重建质量评价
  + 客观评价法：均方误差、信噪比、峰值信噪比
  + 主观评价法：让人去看(

+ 第一代压缩编码技术
  + 统计（熵）编码：对出现几率较大的符号赋一个短码字，出现几率小的赋长码字
  + 预测编码：基于图像数据的空间/时间冗余性，用相邻已知像素来预测当前像素，对预测误差进行编码
  + 变换编码：将空间域上的图像经过正交变换映射到另一变换域，将图像大部分能量集中到少数几个变换系数

+ 第二代压缩编码技术
  
  + 神经网络、分形编码、小波变换编码...



### 行程长度编码

+ 基本思想：检测符号序列中连续重复出现的符号，并使用其长度(run length)进行表示。
+ 行程长度（符号重复次数）>3时才有效益



### 变长度编码（会考试哈）

> 熵编码
>
> 利用符号元出现的概率不同，进行编码（概率大的进行较短的码长编码，保证平均码长较小）

+ Shannon-Fano编码：
  1. 统计每个符号的出现概率，从大到小排序
  2. 切分。分成两个子集，两子集概率和尽可能接近，给前一个子集赋值为0，后一个赋值为1
  3. 重复2. 使得每个子集只有一个符号
  4. 将每个元素所属子集的值依次串起来即得到对应编码
+ Huffman编码：
  1. 统计每个符号的出现概率，从大到小排序
  2. 合并最小的两个符号成为一个节点，累加其概率，重新排序
  3. 重复2. 直到将所有节点合并为1个结点
  4. 从根节点开始，两个子节点中概率大的分配0，小概率的分配1（根节点不用加哈）
  5. 以从根节点往叶节点的方向将值串起来即得到叶节点对应的编码
+ shanno-Fano编码与Huffman编码对同一信号源可能不唯一
  + 平均码长都一样，但尽量选择最长最短码字之差最小的。即假如有多种节点相加的方案时，下一次选择的两个节点尽量是**最近没用过的**
+ Huffman编码分析
  + 每个编码均不会是其他码的前缀
+ 自适应Huffman编码
  + 不统计符号概率，不存储码表
  + 随着符号流的读入不断地建Huffman树
  + 每个符号对应的Huffman编码是在变化的
  + 左分支小于等于右分支
  + 与+1前节点数相等的节点换，节点A与多个子节点可切换时，节点A要与距其拓扑距离最远的节点切换

### 字典编码

+ LWZ编码：给出现过的符号序列编码
  + 编码：首先Buffer为空
    1. 读一个字符A，放入buffer，buffer内容为n+A
    2. 若n+A在字典里，就继续读下一个字符；如果不在，那就把n+A送到字典里，将n输出
  + 解码
  + 有bug（当找不到解码时字典的时候有bug），ppt中给出了改正

### 算数编码

+ 编码思想：把一串符号作为一个编码单位，使得编码效率得到提高（平均码长更接近信息熵）

  + 对于由m个符号组成一个符号串的符号串序列：S1 S2 ······ Sm，把每个符号串映射到［0，1）中的一个半开子区间。（详细步骤见ppt）然后在这个区间中挑一个二进制最短的数作为其编码
  + 找一个二进制位数最短，并且大小符合要求的数作为编码

+ 解码：

  1. i=1
  2. 看编码位于哪个字母的区间，即得到第一个解码的字母

  3. 编码减掉第i个字母的下限，再除以该字母的区间宽度，得到的数落在哪个字母的区间，就解码得到哪个字母
  4. 返回2，直到解码符号达到符号串长度上限





